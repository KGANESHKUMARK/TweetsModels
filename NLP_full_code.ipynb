{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted glove.6B.50d.txt from the zip file.\n",
      "Loaded 400000 word vectors from glove.6B.50d.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|██████████| 442/442 [00:00<00:00, 8505.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: 86821 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Processing Data: 100%|██████████| 442/442 [00:00<00:00, 6148.92it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset Loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    110\u001b[0m evaluate_model(eval_script_path, dataset_path, predictions_path)\n",
      "Cell \u001b[1;32mIn[7], line 81\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[1;34m(model_dir, dataset_path, output_path)\u001b[0m\n\u001b[0;32m     79\u001b[0m squad_data \u001b[38;5;241m=\u001b[39m load_squad_dataset(dataset_path)\n\u001b[0;32m     80\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m \u001b[43msquad_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m paragraph \u001b[38;5;129;01min\u001b[39;00m topic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparagraphs\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     83\u001b[0m         context \u001b[38;5;241m=\u001b[39m paragraph[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import subprocess\n",
    "\n",
    "# Define paths\n",
    "glove_zip_path = r\"D:\\BITS\\Classes\\Sem3\\NLP Applications\\Assignment\\Code\\glove.6B.zip\"\n",
    "glove_file_name = \"glove.6B.50d.txt\"\n",
    "dataset_path = \"train-v2.0.json\"\n",
    "model_dir = \"fine_tuned_distilbert\"\n",
    "predictions_path = \"predictions.json\"\n",
    "eval_script_path = r\"c:\\Users\\Ganesh AI\\Downloads\\evaluate-v2.0.py\"\n",
    "\n",
    "# Step 1: Download and Extract GloVe Embeddings\n",
    "def download_and_extract_glove(zip_path, file_name):\n",
    "    glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading GloVe embeddings...\")\n",
    "        response = requests.get(glove_url)\n",
    "        with open(zip_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        if file_name not in zip_ref.namelist():\n",
    "            raise FileNotFoundError(f\"{file_name} not found in the zip file.\")\n",
    "        zip_ref.extract(file_name, os.path.dirname(zip_path))\n",
    "        print(f\"Extracted {file_name} from the zip file.\")\n",
    "\n",
    "# Step 2: Load GloVe Embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Step 3: Load and Limit the Dataset\n",
    "def load_squad_dataset(file_path, limit=100):\n",
    "    with open(file_path, 'r') as file:\n",
    "        squad_data = json.load(file)\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "    count = 0\n",
    "    \n",
    "    if 'data' not in squad_data:\n",
    "        raise KeyError(\"'data' key not found in the JSON object\")\n",
    "    \n",
    "    # print(squad_data['data'][0])\n",
    "    for topic in tqdm(squad_data['data'], desc=\"Processing Data\"):\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                if not qa['is_impossible']:  # Include only answerable questions\n",
    "                    question = qa['question']\n",
    "                    answer = qa['answers'][0]  # Take the first answer\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"context\": contexts,\n",
    "        \"question\": questions,\n",
    "        \"answer_text\": [ans['text'] for ans in answers],\n",
    "        \"answer_start\": [ans['answer_start'] for ans in answers]\n",
    "    })\n",
    "\n",
    "# Step 4: Generate Predictions Using the Fine-Tuned Model\n",
    "def generate_predictions(model_dir, dataset_path, output_path):\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model_dir, tokenizer=model_dir)\n",
    "    squad_data = load_squad_dataset(dataset_path)\n",
    "    predictions = {}\n",
    "    for topic in squad_data['data']:\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                if not qa['is_impossible']:\n",
    "                    answer = qa_pipeline(question=question, context=context)\n",
    "                    predictions[qa['id']] = answer['answer']\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(predictions, file)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "def evaluate_model(eval_script_path, dataset_path, predictions_path):\n",
    "    subprocess.run([\"python\", eval_script_path, dataset_path, predictions_path])\n",
    "\n",
    "# Execute the steps\n",
    "download_and_extract_glove(glove_zip_path, glove_file_name)\n",
    "glove_embeddings = load_glove_embeddings(os.path.join(os.path.dirname(glove_zip_path), glove_file_name))\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from {glove_file_name}.\")\n",
    "\n",
    "# Limit the dataset to 100 entries\n",
    "df = load_squad_dataset(dataset_path, limit=1)\n",
    "print(f\"Dataset Loaded: {len(df)} examples\")\n",
    "\n",
    "# Generate predictions\n",
    "generate_predictions(model_dir, dataset_path, predictions_path)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(eval_script_path, dataset_path, predictions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys in the JSON object: dict_keys(['version', 'data'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|██████████| 442/442 [00:00<00:00, 8093.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: 86821 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17713</th>\n",
       "      <td>Another theory describes its derivation from a...</td>\n",
       "      <td>WHen did they find the polish kingdom?</td>\n",
       "      <td>about the fifth century.</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50658</th>\n",
       "      <td>When John DeStefano, Jr., became mayor of New ...</td>\n",
       "      <td>When was the last time New Haven had a profess...</td>\n",
       "      <td>2009</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48524</th>\n",
       "      <td>According to the International Organization fo...</td>\n",
       "      <td>How much money did overseas Nigerians send hom...</td>\n",
       "      <td>USD 2.3 billion</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84486</th>\n",
       "      <td>In 1797, Patrick Colquhoun was able to persuad...</td>\n",
       "      <td>Where did the West Indies merchants in London ...</td>\n",
       "      <td>docks</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65100</th>\n",
       "      <td>The Arabic term ijāzat al-tadrīs was awarded t...</td>\n",
       "      <td>What did earning the ijazat al-tadris award st...</td>\n",
       "      <td>licence to teach</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "17713  Another theory describes its derivation from a...   \n",
       "50658  When John DeStefano, Jr., became mayor of New ...   \n",
       "48524  According to the International Organization fo...   \n",
       "84486  In 1797, Patrick Colquhoun was able to persuad...   \n",
       "65100  The Arabic term ijāzat al-tadrīs was awarded t...   \n",
       "\n",
       "                                                question  \\\n",
       "17713             WHen did they find the polish kingdom?   \n",
       "50658  When was the last time New Haven had a profess...   \n",
       "48524  How much money did overseas Nigerians send hom...   \n",
       "84486  Where did the West Indies merchants in London ...   \n",
       "65100  What did earning the ijazat al-tadris award st...   \n",
       "\n",
       "                    answer_text  answer_start  \n",
       "17713  about the fifth century.           836  \n",
       "50658                      2009           575  \n",
       "48524           USD 2.3 billion           158  \n",
       "84486                     docks           165  \n",
       "65100          licence to teach           150  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# Load the dataset\n",
    "def load_squad_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        squad_data = json.load(file)\n",
    "    return squad_data\n",
    "\n",
    "# Extract questions, contexts, and answers\n",
    "def extract_features(squad_data):\n",
    "    # Debugging: Print the keys of the JSON object\n",
    "    print(\"Top-level keys in the JSON object:\", squad_data.keys())\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "\n",
    "    for topic in tqdm(squad_data['data'], desc=\"Processing Data\"):\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                if not qa['is_impossible']:  # Include only answerable questions\n",
    "                    question = qa['question']\n",
    "                    answer = qa['answers'][0]  # Take the first answer\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"context\": contexts,\n",
    "        \"question\": questions,\n",
    "        \"answer_text\": [ans['text'] for ans in answers],\n",
    "        \"answer_start\": [ans['answer_start'] for ans in answers]\n",
    "    })\n",
    "\n",
    "# File path to the SQuAD dataset\n",
    "dataset_path = \"train-v2.0.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not Path(dataset_path).is_file():\n",
    "    response = requests.get(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    if response.status_code == 200:\n",
    "        with open(dataset_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Unable to download the file from the URL\")\n",
    "\n",
    "squad_data = load_squad_dataset(dataset_path)\n",
    "\n",
    "# Extract features\n",
    "df = extract_features(squad_data)\n",
    "\n",
    "# Display a sample\n",
    "print(f\"Dataset Loaded: {len(df)} examples\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON structure: {\n",
      "  \"version\": \"v2.0\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"title\": \"Beyonc\\u00e9\",\n",
      "      \"paragraphs\": [\n",
      "        {\n",
      "          \"qas\": [\n",
      "            {\n",
      "              \"question\": \"When did Beyonce start becoming popular?\",\n",
      "              \"id\": \"56be85543aeaaa14008c9063\",\n",
      "              \"answers\": [\n",
      "                {\n",
      "                  \"text\": \"in the late 1990s\",\n",
      "                  \"answer_start\": 269\n",
      "                }\n",
      "              ],\n",
      "              \"is_impossible\": false\n",
      "            },\n",
      "            {\n",
      "              \"question\": \"What areas did Beyonce compete in when she was growing up?\",\n",
      "              \"id\": \"56be85543aeaaa14008c9065\",\n",
      "              \"answers\": [\n",
      "                {\n",
      "                  \"text\": \"singing and dancing\",\n",
      "                  \"answer_start\": 207\n",
      "                }\n",
      "              ],\n",
      "              \"is_impossible\": false\n",
      "            },\n",
      "            {\n",
      "              \"question\": \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
      "              \"id\": \"56be85543aeaaa14008c9066\",\n",
      "   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data: 100%|██████████| 442/442 [00:00<00:00, 7853.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: 86821 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57409</th>\n",
       "      <td>Nasser's regional position changed unexpectedl...</td>\n",
       "      <td>What country experienced a coup in 1962?</td>\n",
       "      <td>North Yemen</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80413</th>\n",
       "      <td>Mosaic has a long history, starting in Mesopot...</td>\n",
       "      <td>The Norman kingdomwas in what italian city state?</td>\n",
       "      <td>Sicily</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41324</th>\n",
       "      <td>In 2010, 6.9% of the population (1,269,765) co...</td>\n",
       "      <td>What percentage of the population considers th...</td>\n",
       "      <td>6.9% of the population (1,269,765) considered ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70010</th>\n",
       "      <td>President Richard Nixon declared current speci...</td>\n",
       "      <td>What Congress called for the drafting of the E...</td>\n",
       "      <td>the 93rd United States Congress</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12788</th>\n",
       "      <td>Valencian is classified as a Western dialect, ...</td>\n",
       "      <td>What forms are mutually intelligible?</td>\n",
       "      <td>Catalan and Valencian</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "57409  Nasser's regional position changed unexpectedl...   \n",
       "80413  Mosaic has a long history, starting in Mesopot...   \n",
       "41324  In 2010, 6.9% of the population (1,269,765) co...   \n",
       "70010  President Richard Nixon declared current speci...   \n",
       "12788  Valencian is classified as a Western dialect, ...   \n",
       "\n",
       "                                                question  \\\n",
       "57409           What country experienced a coup in 1962?   \n",
       "80413  The Norman kingdomwas in what italian city state?   \n",
       "41324  What percentage of the population considers th...   \n",
       "70010  What Congress called for the drafting of the E...   \n",
       "12788              What forms are mutually intelligible?   \n",
       "\n",
       "                                             answer_text  answer_start  \n",
       "57409                                        North Yemen           135  \n",
       "80413                                             Sicily           481  \n",
       "41324  6.9% of the population (1,269,765) considered ...             9  \n",
       "70010                    the 93rd United States Congress           101  \n",
       "12788                              Catalan and Valencian           190  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# Load the dataset\n",
    "def load_squad_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        squad_data = json.load(file)\n",
    "    return squad_data\n",
    "\n",
    "# Extract questions, contexts, and answers\n",
    "def extract_features(squad_data):\n",
    "    # Debugging: Print the entire JSON object\n",
    "    print(\"JSON structure:\", json.dumps(squad_data, indent=2)[:1000])  # Print the first 1000 characters for readability\n",
    "    \n",
    "    # Check if 'data' key exists\n",
    "    if 'data' not in squad_data:\n",
    "        raise KeyError(\"'data' key not found in the JSON object\")\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "\n",
    "    for topic in tqdm(squad_data['data'], desc=\"Processing Data\"):\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                if not qa['is_impossible']:  # Include only answerable questions\n",
    "                    question = qa['question']\n",
    "                    answer = qa['answers'][0]  # Take the first answer\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"context\": contexts,\n",
    "        \"question\": questions,\n",
    "        \"answer_text\": [ans['text'] for ans in answers],\n",
    "        \"answer_start\": [ans['answer_start'] for ans in answers]\n",
    "    })\n",
    "\n",
    "# File path to the SQuAD dataset\n",
    "dataset_path = \"train-v2.0.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not Path(dataset_path).is_file():\n",
    "    response = requests.get(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
    "    if response.status_code == 200:\n",
    "        with open(dataset_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Unable to download the file from the URL\")\n",
    "\n",
    "squad_data = load_squad_dataset(dataset_path)\n",
    "\n",
    "# Extract features\n",
    "df = extract_features(squad_data)\n",
    "\n",
    "# Display a sample\n",
    "print(f\"Dataset Loaded: {len(df)} examples\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted glove.6B.50d.txt from the zip file.\n",
      "Loaded 400000 word vectors from glove.6B.50d.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Data:   0%|          | 1/442 [00:00<00:04, 103.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded: 1000 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Processing Data:   0%|          | 0/442 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import subprocess\n",
    "\n",
    "# Define paths\n",
    "glove_zip_path = r\"D:\\BITS\\Classes\\Sem3\\NLP Applications\\Assignment\\Code\\glove.6B.zip\"\n",
    "glove_file_name = \"glove.6B.50d.txt\"\n",
    "dataset_path = \"train-v2.0.json\"\n",
    "model_dir = \"fine_tuned_distilbert\"\n",
    "predictions_path = \"predictions.json\"\n",
    "eval_script_path = r\"c:\\Users\\Ganesh AI\\Downloads\\evaluate-v2.0.py\"\n",
    "\n",
    "# Step 1: Download and Extract GloVe Embeddings\n",
    "def download_and_extract_glove(zip_path, file_name):\n",
    "    glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Downloading GloVe embeddings...\")\n",
    "        response = requests.get(glove_url)\n",
    "        with open(zip_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        if file_name not in zip_ref.namelist():\n",
    "            raise FileNotFoundError(f\"{file_name} not found in the zip file.\")\n",
    "        zip_ref.extract(file_name, os.path.dirname(zip_path))\n",
    "        print(f\"Extracted {file_name} from the zip file.\")\n",
    "\n",
    "# Step 2: Load GloVe Embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Step 3: Load and Limit the Dataset\n",
    "def load_squad_dataset(file_path, limit=100):\n",
    "    with open(file_path, 'r') as file:\n",
    "        squad_data = json.load(file)\n",
    "    \n",
    "    contexts, questions, answers = [], [], []\n",
    "    count = 0\n",
    "    \n",
    "    if 'data' not in squad_data:\n",
    "        raise KeyError(\"'data' key not found in the JSON object\")\n",
    "    \n",
    "    for topic in tqdm(squad_data['data'], desc=\"Processing Data\"):\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                if not qa['is_impossible']:  # Include only answerable questions\n",
    "                    question = qa['question']\n",
    "                    answer = qa['answers'][0]  # Take the first answer\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "                    count += 1\n",
    "                    if count >= limit:\n",
    "                        return pd.DataFrame({\n",
    "                            \"context\": contexts,\n",
    "                            \"question\": questions,\n",
    "                            \"answer_text\": [ans['text'] for ans in answers],\n",
    "                            \"answer_start\": [ans['answer_start'] for ans in answers]\n",
    "                        })\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"context\": contexts,\n",
    "        \"question\": questions,\n",
    "        \"answer_text\": [ans['text'] for ans in answers],\n",
    "        \"answer_start\": [ans['answer_start'] for ans in answers]\n",
    "    })\n",
    "\n",
    "# Step 4: Generate Predictions Using the Fine-Tuned Model\n",
    "def generate_predictions(model_dir, dataset_path, output_path):\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model_dir, tokenizer=model_dir)\n",
    "    squad_data = load_squad_dataset(dataset_path)\n",
    "    predictions = {}\n",
    "    for index, row in squad_data.iterrows():\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        answer = qa_pipeline(question=question, context=context)\n",
    "        predictions[index] = answer['answer']\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(predictions, file)\n",
    "    print(f\"Predictions saved to {output_path}\")\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "def evaluate_model(eval_script_path, dataset_path, predictions_path):\n",
    "    subprocess.run([\"python\", eval_script_path, dataset_path, predictions_path])\n",
    "\n",
    "# Execute the steps\n",
    "download_and_extract_glove(glove_zip_path, glove_file_name)\n",
    "glove_embeddings = load_glove_embeddings(os.path.join(os.path.dirname(glove_zip_path), glove_file_name))\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors from {glove_file_name}.\")\n",
    "\n",
    "# Limit the dataset to 100 entries\n",
    "df = load_squad_dataset(dataset_path, limit=1000)\n",
    "print(f\"Dataset Loaded: {len(df)} examples\")\n",
    "\n",
    "# Generate predictions\n",
    "generate_predictions(model_dir, dataset_path, predictions_path)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(eval_script_path, dataset_path, predictions_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
