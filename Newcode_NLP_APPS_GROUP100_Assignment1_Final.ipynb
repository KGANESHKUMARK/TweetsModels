{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                                           NLP Applications Assignment 1\n",
    "\n",
    "##                                                       Group 80\n",
    "\n",
    "##                                               Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "\n",
       "    <style>\n",
       "        .table {\n",
       "            display: table;\n",
       "            width: 70%;\n",
       "            border-collapse: collapse;\n",
       "        }\n",
       "        .table-row {\n",
       "            display: table-row;\n",
       "        }\n",
       "        .table-cell {\n",
       "            display: table-cell;\n",
       "            border: 1px solid #dddddd;\n",
       "            padding: 8px;\n",
       "        }\n",
       "        .header {\n",
       "            background-color: #fffff;\n",
       "            font-weight: bold;\n",
       "            font-size: 100%;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       " <h3>Group Members Name with Student ID:</h3>\n",
       "\n",
       "<div class=\"table\" style=\"float: left\">\n",
       "\n",
       "    <div class=\"table-row header\">\n",
       "        <div class=\"table-cell\">BITS ID</div>\n",
       "        <div class=\"table-cell\">Name</div>\n",
       "        <div class=\"table-cell\">BITS ID</div>\n",
       "        <div class=\"table-cell\">Email ID</div>\n",
       "        <div class=\"table-cell\">Contribution</div>\n",
       "    </div>\n",
       "    <div class=\"table-row\">\n",
       "        <div class=\"table-cell\">1.</div>\n",
       "        <div class=\"table-cell\">GANESHKUMAR KARUPPAIAH</div>\n",
       "        <div class=\"table-cell\">2023aa05600</div>\n",
       "        <div class=\"table-cell\">2023aa05600@wilp.bits-pilani.ac.in</div>\n",
       "        <div class=\"table-cell\">100%</div>\n",
       "    </div>\n",
       "    <div class=\"table-row\">\n",
       "    <div class=\"table-cell\">2.</div>\n",
       "        <div class=\"table-cell\">KOLLURI VENKATESWARA SWAROOP</div>\n",
       "        <div class=\"table-cell\">2023aa05945</div>\n",
       "        <div class=\"table-cell\">2023aa05945@wilp.bits-pilani.ac.in</div>\n",
       "        <div class=\"table-cell\">100%</div>\n",
       "    </div>\n",
       "    <div class=\"table-row\">\n",
       "    <div class=\"table-cell\">3.</div>\n",
       "        <div class=\"table-cell\">RAJESH J</div>\n",
       "        <div class=\"table-cell\">2023aa05859</div>\n",
       "        <div class=\"table-cell\">2023aa05859@wilp.bits-pilani.ac.in</div>\n",
       "        <div class=\"table-cell\">100%</div>\n",
       "    </div>\n",
       "    <div class=\"table-row\">\n",
       "    <div class=\"table-cell\">4.</div>\n",
       "        <div class=\"table-cell\">VIJAYA LAKSHMI R</div>\n",
       "        <div class=\"table-cell\">2023aa05341</div>\n",
       "        <div class=\"table-cell\">2023aa05341@wilp.bits-pilani.ac.in</div>\n",
       "        <div class=\"table-cell\">100%</div>\n",
       "    </div>\n",
       "    <div class=\"table-cell\">5.</div>\n",
       "        <div class=\"table-cell\">BHALANI AMIT KISHORKUMAR</div>\n",
       "        <div class=\"table-cell\">2023ab05169</div>\n",
       "        <div class=\"table-cell\">2023ab05169@wilp.bits-pilani.ac.in</div>\n",
       "        <div class=\"table-cell\">100%</div>\n",
       "    </div>\n",
       "</div>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### Group Members Name with Student ID:#\n",
    "from IPython.display import display, HTML\n",
    "html_content = \"\"\"\n",
    "\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "\n",
    "    <style>\n",
    "        .table {\n",
    "            display: table;\n",
    "            width: 70%;\n",
    "            border-collapse: collapse;\n",
    "        }\n",
    "        .table-row {\n",
    "            display: table-row;\n",
    "        }\n",
    "        .table-cell {\n",
    "            display: table-cell;\n",
    "            border: 1px solid #dddddd;\n",
    "            padding: 8px;\n",
    "        }\n",
    "        .header {\n",
    "            background-color: #fffff;\n",
    "            font-weight: bold;\n",
    "            font-size: 100%;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    " <h3>Group Members Name with Student ID:</h3>\n",
    "\n",
    "<div class=\"table\" style=\"float: left\">\n",
    "\n",
    "    <div class=\"table-row header\">\n",
    "        <div class=\"table-cell\">BITS ID</div>\n",
    "        <div class=\"table-cell\">Name</div>\n",
    "        <div class=\"table-cell\">BITS ID</div>\n",
    "        <div class=\"table-cell\">Email ID</div>\n",
    "        <div class=\"table-cell\">Contribution</div>\n",
    "    </div>\n",
    "    <div class=\"table-row\">\n",
    "        <div class=\"table-cell\">1.</div>\n",
    "        <div class=\"table-cell\">GANESHKUMAR KARUPPAIAH</div>\n",
    "        <div class=\"table-cell\">2023aa05600</div>\n",
    "        <div class=\"table-cell\">2023aa05600@wilp.bits-pilani.ac.in</div>\n",
    "        <div class=\"table-cell\">100%</div>\n",
    "    </div>\n",
    "    <div class=\"table-row\">\n",
    "    <div class=\"table-cell\">2.</div>\n",
    "        <div class=\"table-cell\">KOLLURI VENKATESWARA SWAROOP</div>\n",
    "        <div class=\"table-cell\">2023aa05945</div>\n",
    "        <div class=\"table-cell\">2023aa05945@wilp.bits-pilani.ac.in</div>\n",
    "        <div class=\"table-cell\">100%</div>\n",
    "    </div>\n",
    "    <div class=\"table-row\">\n",
    "    <div class=\"table-cell\">3.</div>\n",
    "        <div class=\"table-cell\">RAJESH J</div>\n",
    "        <div class=\"table-cell\">2023aa05859</div>\n",
    "        <div class=\"table-cell\">2023aa05859@wilp.bits-pilani.ac.in</div>\n",
    "        <div class=\"table-cell\">100%</div>\n",
    "    </div>\n",
    "    <div class=\"table-row\">\n",
    "    <div class=\"table-cell\">4.</div>\n",
    "        <div class=\"table-cell\">VIJAYA LAKSHMI R</div>\n",
    "        <div class=\"table-cell\">2023aa05341</div>\n",
    "        <div class=\"table-cell\">2023aa05341@wilp.bits-pilani.ac.in</div>\n",
    "        <div class=\"table-cell\">100%</div>\n",
    "    </div>\n",
    "    <div class=\"table-cell\">5.</div>\n",
    "        <div class=\"table-cell\">BHALANI AMIT KISHORKUMAR</div>\n",
    "        <div class=\"table-cell\">2023ab05169</div>\n",
    "        <div class=\"table-cell\">2023ab05169@wilp.bits-pilani.ac.in</div>\n",
    "        <div class=\"table-cell\">100%</div>\n",
    "    </div>\n",
    "</div>\n",
    "</html>\n",
    "\"\"\"\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including transformers, torch, numpy, pandas, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.47.1\n",
      "Torch version: 2.5.1+cpu\n",
      "Numpy version: 1.26.4\n",
      "Pandas version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "# Import the necessary libraries for the notebook\n",
    "import transformers  # For BERT model and tokenization\n",
    "import torch  # For tensor operations and model training\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "\n",
    "# Print the versions of the libraries to ensure compatibility\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset and Extract Features and Labels\n",
    "Load the dataset from the provided URL, extract features and labels, and take a sample of 100 data points for initial processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question 1: When did Beyonce start becoming popular?\n",
      "Answer 1: in the late 1990s\n",
      "\n",
      "Context 2: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question 2: What areas did Beyonce compete in when she was growing up?\n",
      "Answer 2: singing and dancing\n",
      "\n",
      "Context 3: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question 3: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer 3: 2003\n",
      "\n",
      "Context 4: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question 4: In what city and state did Beyonce  grow up? \n",
      "Answer 4: Houston, Texas\n",
      "\n",
      "Context 5: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question 5: In which decade did Beyonce become famous?\n",
      "Answer 5: late 1990s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset and Extract Features and Labels\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Load the dataset from the provided URL\n",
    "url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extract features and labels from the dataset\n",
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for article in data['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        context = paragraph['context']\n",
    "        for qa in paragraph['qas']:\n",
    "            question = qa['question']\n",
    "            if 'answers' in qa and len(qa['answers']) > 0:\n",
    "                answer = qa['answers'][0]['text']\n",
    "                contexts.append(context)\n",
    "                questions.append(question)\n",
    "                answers.append(answer)\n",
    "\n",
    "# Take a sample of 100 data points for initial processing\n",
    "sample_size = 100\n",
    "contexts_sample = contexts[:sample_size]\n",
    "questions_sample = questions[:sample_size]\n",
    "answers_sample = answers[:sample_size]\n",
    "\n",
    "# Print the first 5 samples to verify\n",
    "for i in range(5):\n",
    "    print(f\"Context {i+1}: {contexts_sample[i]}\")\n",
    "    print(f\"Question {i+1}: {questions_sample[i]}\")\n",
    "    print(f\"Answer {i+1}: {answers_sample[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Features and Labels using Sentence Embedding Techniques\n",
    "Use sentence embedding techniques like sentence2vec or doc2vec to vectorize the features and labels. Choose the best embedding technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80\n",
      "Validation set size: 20\n"
     ]
    }
   ],
   "source": [
    "# Vectorize Features and Labels using Sentence Embedding Techniques\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Combine contexts and questions for embedding\n",
    "combined_texts = [context + \" \" + question for context, question in zip(contexts_sample, questions_sample)]\n",
    "\n",
    "# Tag documents for Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(combined_texts)]\n",
    "\n",
    "# Initialize and train the Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=40)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Vectorize contexts and questions\n",
    "context_vectors = [model.infer_vector(context.split()) for context in contexts_sample]\n",
    "question_vectors = [model.infer_vector(question.split()) for question in questions_sample]\n",
    "\n",
    "# Combine context and question vectors\n",
    "features = [np.concatenate((context_vector, question_vector)) for context_vector, question_vector in zip(context_vectors, question_vectors)]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(features, answers_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and validation sets\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune BERT Model\n",
    "Fine-tune the BERT model using the vectorized features and labels from the embeddings defined in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Ganesh AI\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Ganesh AI\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.247320508956909\n",
      "Epoch 2, Loss: 5.96860990524292\n",
      "Epoch 3, Loss: 5.774764966964722\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer: \n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer: \n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer: \n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer: \n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune BERT Model\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the input data\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for context, question in zip(contexts_sample, questions_sample):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        question, context,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Convert answers to tensors\n",
    "answer_starts = [context.find(answer) for context, answer in zip(contexts_sample, answers_sample)]\n",
    "answer_ends = [start + len(answer) for start, answer in zip(answer_starts, answers_sample)]\n",
    "answer_starts = torch.tensor(answer_starts)\n",
    "answer_ends = torch.tensor(answer_ends)\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "dataset = TensorDataset(input_ids, attention_masks, answer_starts, answer_ends)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Load the BERT model for question answering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "model.train()\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids, b_attention_masks, b_start_positions, b_end_positions = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_attention_masks,\n",
    "            start_positions=b_start_positions,\n",
    "            end_positions=b_end_positions\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_train_loss}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_bert')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert')\n",
    "\n",
    "# Print answers for 5 context and questions\n",
    "model.eval()\n",
    "for i in range(5):\n",
    "    context = contexts_sample[i]\n",
    "    question = questions_sample[i]\n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference: Answering Questions\n",
    "The model should take a question and context as input and provide answers. Print answers for 5 contexts and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.\n",
      "Question: What is statistics?\n",
      "Answer: [CLS] what is statistics ? [SEP] statistics is the discipline that concerns the collection , organization , analysis , interpretation , and presentation of data .\n",
      "\n",
      "Context: Descriptive statistics is a branch of statistics that summarizes and describes the features of a dataset.\n",
      "Question: Where can descriptive statistics be used?\n",
      "Answer: [CLS] where can descriptive statistics be used ? [SEP] descriptive statistics is a branch of statistics that summarizes and describes the features of a dataset .\n",
      "\n",
      "Context: Inferential statistics is used to draw conclusions and make predictions about a population based on a sample of data.\n",
      "Question: How to draw meaningful conclusions?\n",
      "Answer: ##ferential statistics is used to draw conclusions and make predictions about a population based on a sample of data .\n",
      "\n",
      "Context: Probability theory is a branch of mathematics concerned with the analysis of random phenomena.\n",
      "Question: What is probability theory?\n",
      "Answer: [CLS] what is probability theory ? [SEP] probability theory is a branch of mathematics concerned with the analysis of random phenomena .\n",
      "\n",
      "Context: A statistical hypothesis is a conjecture about a population parameter that can be tested using statistical methods.\n",
      "Question: What is a statistical hypothesis?\n",
      "Answer: [CLS] what is a statistical hypothesis ? [SEP] a statistical hypothesis is a conjecture about a population parameter that can be tested using statistical methods .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Inference: Answering Questions\n",
    "\n",
    "# Load the fine-tuned BERT model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained('./fine_tuned_bert')\n",
    "tokenizer = BertTokenizer.from_pretrained('./fine_tuned_bert')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the contexts and questions for inference\n",
    "contexts_inference = [\n",
    "    \"Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data.\",\n",
    "    \"Descriptive statistics is a branch of statistics that summarizes and describes the features of a dataset.\",\n",
    "    \"Inferential statistics is used to draw conclusions and make predictions about a population based on a sample of data.\",\n",
    "    \"Probability theory is a branch of mathematics concerned with the analysis of random phenomena.\",\n",
    "    \"A statistical hypothesis is a conjecture about a population parameter that can be tested using statistical methods.\"\n",
    "]\n",
    "\n",
    "questions_inference = [\n",
    "    \"What is statistics?\",\n",
    "    \"Where can descriptive statistics be used?\",\n",
    "    \"How to draw meaningful conclusions?\",\n",
    "    \"What is probability theory?\",\n",
    "    \"What is a statistical hypothesis?\"\n",
    "]\n",
    "\n",
    "# Print answers for the given contexts and questions\n",
    "for context, question in zip(contexts_inference, questions_inference):\n",
    "    # Tokenize the input context and question\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    # Get the model's output\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the tokens with the highest start and end scores\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores) + 1\n",
    "\n",
    "    # Convert the tokens to the answer string\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))\n",
    "\n",
    "    # Print the context, question, and answer\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Use the provided evaluation script to evaluate the fine-tuned model. Provide context from the Wikipedia link and ask the specified questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate-v2.0.py already exists.\n",
      "Training set size with GloVe: 80\n",
      "Validation set size with GloVe: 20\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Model\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Download the evaluation script\n",
    "evaluation_script_url = \"https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\"\n",
    "evaluation_script_path = \"evaluate-v2.0.py\"\n",
    "\n",
    "\n",
    "\n",
    "# Check if the file exists locally\n",
    "if not os.path.exists(evaluation_script_path):\n",
    "    print(f\"{evaluation_script_path} not found. Downloading from {file_url}...\")\n",
    "    response = requests.get(evaluation_script_url)\n",
    "    with open(evaluation_script_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {evaluation_script_path}.\")\n",
    "else:\n",
    "    print(f\"{evaluation_script_path} already exists.\")\n",
    "\n",
    "\n",
    "# Save the predictions in the required format\n",
    "predictions = {}\n",
    "for i, (context, question) in enumerate(zip(contexts_sample, questions_sample)):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))\n",
    "    predictions[str(i)] = answer\n",
    "\n",
    "# Save predictions to a JSON file\n",
    "predictions_path = \"predictions.json\"\n",
    "with open(predictions_path, 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# Save the ground truth answers in the required format\n",
    "ground_truths = {}\n",
    "for i, answer in enumerate(answers_sample):\n",
    "    ground_truths[str(i)] = answer\n",
    "\n",
    "# Save ground truths to a JSON file\n",
    "ground_truths_path = \"ground_truths.json\"\n",
    "with open(ground_truths_path, 'w') as f:\n",
    "    json.dump(ground_truths, f)\n",
    "\n",
    "# Run the evaluation script\n",
    "subprocess.run([\"python\", evaluation_script_path, ground_truths_path, predictions_path])\n",
    "\n",
    "# Evaluate if BERT model performs better with GloVe embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = \"content/glove.6B.50d.txt\"\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
    "\n",
    "# Vectorize contexts and questions using GloVe\n",
    "context_vectors_glove = [np.mean([glove_vectors[word] for word in context.split() if word in glove_vectors], axis=0) for context in contexts_sample]\n",
    "question_vectors_glove = [np.mean([glove_vectors[word] for word in question.split() if word in glove_vectors], axis=0) for question in questions_sample]\n",
    "\n",
    "# Combine context and question vectors\n",
    "features_glove = [np.concatenate((context_vector, question_vector)) for context_vector, question_vector in zip(context_vectors_glove, question_vectors_glove)]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_glove, X_val_glove, y_train_glove, y_val_glove = train_test_split(features_glove, answers_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and validation sets\n",
    "print(\"Training set size with GloVe:\", len(X_train_glove))\n",
    "print(\"Validation set size with GloVe:\", len(X_val_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate BERT Model with GloVe Embeddings\n",
    "Evaluate if the BERT model performs better when GloVe embeddings are used to embed the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size with GloVe: 80\n",
      "Validation set size with GloVe: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ganesh AI\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2673: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Ganesh AI\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss with GloVe: 6.2506085395812985\n",
      "Epoch 2, Loss with GloVe: 6.035017728805542\n",
      "Epoch 3, Loss with GloVe: 5.842369651794433\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: When did Beyonce start becoming popular?\n",
      "Answer with GloVe: start becoming popular ? [SEP] beyonce gi ##selle knowles - carter ( / bi ##ː ##ˈ ##j ##ɒ ##nse ##ɪ / bee - yo ##n - say ) ( born september 4 , 1981 ) is an american singer , songwriter , record producer and actress . born and raised in houston , texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of r & b girl - group destiny ' s child . managed by her father , mathew knowles , the group became one of the world ' s best -\n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: What areas did Beyonce compete in when she was growing up?\n",
      "Answer with GloVe: \n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
      "Answer with GloVe: \n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: In what city and state did Beyonce  grow up? \n",
      "Answer with GloVe: city and state did beyonce grow up ? [SEP] beyonce gi ##selle knowles - carter ( / bi ##ː ##ˈ ##j ##ɒ ##nse ##ɪ / bee - yo ##n - say ) ( born september 4 , 1981 ) is an american singer , songwriter , record producer and actress . born and raised in houston , texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of r & b girl - group destiny ' s child . managed by her father , mathew knowles , the group became one of the world ' s best -\n",
      "\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Question: In which decade did Beyonce become famous?\n",
      "Answer with GloVe: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate if BERT model performs better with GloVe embeddings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = \"content/glove.6B.50d.txt\"\n",
    "glove_vectors = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
    "\n",
    "# Vectorize contexts and questions using GloVe\n",
    "context_vectors_glove = [np.mean([glove_vectors[word] for word in context.split() if word in glove_vectors], axis=0) for context in contexts_sample]\n",
    "question_vectors_glove = [np.mean([glove_vectors[word] for word in question.split() if word in glove_vectors], axis=0) for question in questions_sample]\n",
    "\n",
    "# Combine context and question vectors\n",
    "features_glove = [np.concatenate((context_vector, question_vector)) for context_vector, question_vector in zip(context_vectors_glove, question_vectors_glove)]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train_glove, X_val_glove, y_train_glove, y_val_glove = train_test_split(features_glove, answers_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and validation sets\n",
    "print(\"Training set size with GloVe:\", len(X_train_glove))\n",
    "print(\"Validation set size with GloVe:\", len(X_val_glove))\n",
    "\n",
    "# Fine-tune BERT Model with GloVe embeddings\n",
    "\n",
    "# Tokenize the input data\n",
    "input_ids_glove = []\n",
    "attention_masks_glove = []\n",
    "\n",
    "for context, question in zip(contexts_sample, questions_sample):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        question, context,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids_glove.append(encoded_dict['input_ids'])\n",
    "    attention_masks_glove.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids_glove = torch.cat(input_ids_glove, dim=0)\n",
    "attention_masks_glove = torch.cat(attention_masks_glove, dim=0)\n",
    "\n",
    "# Convert answers to tensors\n",
    "answer_starts_glove = [context.find(answer) for context, answer in zip(contexts_sample, answers_sample)]\n",
    "answer_ends_glove = [start + len(answer) for start, answer in zip(answer_starts_glove, answers_sample)]\n",
    "answer_starts_glove = torch.tensor(answer_starts_glove)\n",
    "answer_ends_glove = torch.tensor(answer_ends_glove)\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "dataset_glove = TensorDataset(input_ids_glove, attention_masks_glove, answer_starts_glove, answer_ends_glove)\n",
    "train_size_glove = int(0.8 * len(dataset_glove))\n",
    "val_size_glove = len(dataset_glove) - train_size_glove\n",
    "train_dataset_glove, val_dataset_glove = torch.utils.data.random_split(dataset_glove, [train_size_glove, val_size_glove])\n",
    "\n",
    "train_dataloader_glove = DataLoader(train_dataset_glove, batch_size=8, shuffle=True)\n",
    "val_dataloader_glove = DataLoader(val_dataset_glove, batch_size=8, shuffle=False)\n",
    "\n",
    "# Load the BERT model for question answering\n",
    "model_glove = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "model_glove.train()\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer_glove = AdamW(model_glove.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs_glove = 3\n",
    "for epoch in range(epochs_glove):\n",
    "    total_loss_glove = 0\n",
    "    for batch in train_dataloader_glove:\n",
    "        b_input_ids, b_attention_masks, b_start_positions, b_end_positions = batch\n",
    "\n",
    "        model_glove.zero_grad()\n",
    "\n",
    "        outputs = model_glove(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_attention_masks,\n",
    "            start_positions=b_start_positions,\n",
    "            end_positions=b_end_positions\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss_glove += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_glove.step()\n",
    "\n",
    "    avg_train_loss_glove = total_loss_glove / len(train_dataloader_glove)\n",
    "    print(f\"Epoch {epoch + 1}, Loss with GloVe: {avg_train_loss_glove}\")\n",
    "\n",
    "# Save the fine-tuned model with GloVe embeddings\n",
    "model_glove.save_pretrained('./fine_tuned_bert_glove')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert_glove')\n",
    "\n",
    "# Print answers for 5 context and questions using the model fine-tuned with GloVe embeddings\n",
    "model_glove.eval()\n",
    "for i in range(5):\n",
    "    context = contexts_sample[i]\n",
    "    question = questions_sample[i]\n",
    "\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention_mask']\n",
    "\n",
    "    outputs = model_glove(input_ids, attention_mask=attention_mask)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer with GloVe: {answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Validation Loss: 5.7748\n",
      "Combined BERT Model with GloVe Validation Loss: 5.8424\n",
      "The BERT model without GloVe embeddings performs better than the Combined BERT model with GloVe embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Print the comparison results\n",
    "print(f\"BERT Model Validation Loss: {avg_train_loss:.4f}\")\n",
    "print(f\"Combined BERT Model with GloVe Validation Loss: {avg_train_loss_glove:.4f}\")\n",
    "\n",
    "# Draw conclusions\n",
    "if avg_train_loss_glove < avg_train_loss:\n",
    "    print(\"The Combined BERT model with GloVe embeddings performs better than the BERT model without GloVe embeddings.\")\n",
    "else:\n",
    "    print(\"The BERT model without GloVe embeddings performs better than the Combined BERT model with GloVe embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Based on the validation losses of both models, we can determine which model performs better. If the validation loss of the combined BERT model with GloVe embeddings is lower than that of the BERT model without GloVe embeddings, it indicates that the combined model performs better. Otherwise, the BERT model without GloVe embeddings performs better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
